{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6868e3bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:33.421963Z",
     "iopub.status.busy": "2023-10-16T20:47:33.421566Z",
     "iopub.status.idle": "2023-10-16T20:47:38.371110Z",
     "shell.execute_reply": "2023-10-16T20:47:38.369747Z"
    },
    "papermill": {
     "duration": 4.958109,
     "end_time": "2023-10-16T20:47:38.373874",
     "exception": false,
     "start_time": "2023-10-16T20:47:33.415765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import  functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa63db0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.385937Z",
     "iopub.status.busy": "2023-10-16T20:47:38.385451Z",
     "iopub.status.idle": "2023-10-16T20:47:38.422566Z",
     "shell.execute_reply": "2023-10-16T20:47:38.421398Z"
    },
    "papermill": {
     "duration": 0.044496,
     "end_time": "2023-10-16T20:47:38.424925",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.380429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oliver_twist = open('/kaggle/input/dickens/dickens/pg730.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb7b875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.434309Z",
     "iopub.status.busy": "2023-10-16T20:47:38.433876Z",
     "iopub.status.idle": "2023-10-16T20:47:38.448612Z",
     "shell.execute_reply": "2023-10-16T20:47:38.447457Z"
    },
    "papermill": {
     "duration": 0.021963,
     "end_time": "2023-10-16T20:47:38.450860",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.428897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting all the characters, like in the n-gram model \n",
    "chars = sorted(list(set(oliver_twist)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467628ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.460642Z",
     "iopub.status.busy": "2023-10-16T20:47:38.460267Z",
     "iopub.status.idle": "2023-10-16T20:47:38.466091Z",
     "shell.execute_reply": "2023-10-16T20:47:38.465068Z"
    },
    "papermill": {
     "duration": 0.013487,
     "end_time": "2023-10-16T20:47:38.468224",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.454737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#a simple tokenizer\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s] #tokenize some characters\n",
    "decode = lambda i: ' '.join([itos[num] for num in i]) #detokenize some integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec85d066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.476808Z",
     "iopub.status.busy": "2023-10-16T20:47:38.476485Z",
     "iopub.status.idle": "2023-10-16T20:47:38.707638Z",
     "shell.execute_reply": "2023-10-16T20:47:38.706175Z"
    },
    "papermill": {
     "duration": 0.238297,
     "end_time": "2023-10-16T20:47:38.710014",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.471717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([84, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
       "        59, 62])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing the entire data set\n",
    "enc = torch.tensor(encode(oliver_twist), dtype=torch.long)\n",
    "enc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da02aee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.720162Z",
     "iopub.status.busy": "2023-10-16T20:47:38.719737Z",
     "iopub.status.idle": "2023-10-16T20:47:38.790723Z",
     "shell.execute_reply": "2023-10-16T20:47:38.789520Z"
    },
    "papermill": {
     "duration": 0.078554,
     "end_time": "2023-10-16T20:47:38.793172",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.714618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting the text into train, test portions\n",
    "train, test = train_test_split(enc, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa4714",
   "metadata": {
    "papermill": {
     "duration": 0.004392,
     "end_time": "2023-10-16T20:47:38.802534",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.798142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When the model is trained on a data set, it essentially samples random chunks of the set and processes them one by one, instead of taking the entire set all at once. That would be computationally expensive and unreasonable. Thus, the procedure is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928b0ec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.815242Z",
     "iopub.status.busy": "2023-10-16T20:47:38.813821Z",
     "iopub.status.idle": "2023-10-16T20:47:38.825806Z",
     "shell.execute_reply": "2023-10-16T20:47:38.824606Z"
    },
    "papermill": {
     "duration": 0.021906,
     "end_time": "2023-10-16T20:47:38.829257",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.807351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for: tensor([63]) the target is: 7\n",
      "for: tensor([63,  7]) the target is: 0\n",
      "for: tensor([63,  7,  0]) the target is: 1\n",
      "for: tensor([63,  7,  0,  1]) the target is: 73\n",
      "for: tensor([63,  7,  0,  1, 73]) the target is: 63\n",
      "for: tensor([63,  7,  0,  1, 73, 63]) the target is: 72\n",
      "for: tensor([63,  7,  0,  1, 73, 63, 72]) the target is: 0\n",
      "for: tensor([63,  7,  0,  1, 73, 63, 72,  0]) the target is: 65\n"
     ]
    }
   ],
   "source": [
    "#the length of the sampled block\n",
    "block_size = 8\n",
    "\n",
    "#an example sample from the training data\n",
    "x = train[:block_size]\n",
    "y = train[1:block_size+1] #the targets for x (x offset by 1)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f'for: {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a68cd6",
   "metadata": {
    "papermill": {
     "duration": 0.003835,
     "end_time": "2023-10-16T20:47:38.837100",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.833265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "However, there is another dimension to care about -- the batch size, the number of sequences (samples) to be processed simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd35e11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.847182Z",
     "iopub.status.busy": "2023-10-16T20:47:38.846088Z",
     "iopub.status.idle": "2023-10-16T20:47:38.873132Z",
     "shell.execute_reply": "2023-10-16T20:47:38.872198Z"
    },
    "papermill": {
     "duration": 0.034714,
     "end_time": "2023-10-16T20:47:38.875463",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.840749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4 #the number of sequences to be processed at the same time \n",
    "\n",
    "#a function which returns the 4 random sequences of context length 8\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #stacking the 1D tensors as rows into a matrix\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #same thing for ys\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xt, yt = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a547e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.886020Z",
     "iopub.status.busy": "2023-10-16T20:47:38.885428Z",
     "iopub.status.idle": "2023-10-16T20:47:38.966674Z",
     "shell.execute_reply": "2023-10-16T20:47:38.965255Z"
    },
    "papermill": {
     "duration": 0.08993,
     "end_time": "2023-10-16T20:47:38.969268",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.879338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " q I r G b s O O - p $ % i r l l 4 ] V M V ( m $ [ ] U G 3 F i i I % i s # G s b 6 8 r - g V [ f ( V S g J 9 ] _ , m i 1 l $   \n",
      " / m s # . e ] @ q j P ] z n 2 C ' a 6 $ C h A [ v G x o y i ( Q h 5 r i\n"
     ]
    }
   ],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #creating a lookup table, just like in makemore #1\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx, targets -- (B,T) tensors of integers                                 #Time = block_size\n",
    "        logits = self.token_embedding_table(idx) # (a tensor of dimensions (Batch x Time x Channel))\n",
    "        #logits -- the logs of probabilities of the characters being the next ones after some other characters\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "        \n",
    "            #reshaping the tensor becaus of torch specifics \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #cross entropy = negative log likelihood\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "                      #idx - the current context of some characters in the current batch \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx - a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #getting the predictions \n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step (pluck out the last value in the Time dimension, pytorch notation)\n",
    "            logits = logits[:, -1, :] #transforms into (B, C)\n",
    "            #apply the softmaax activation to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            #append the sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)#\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "bi = BigramModel(vocab_size)\n",
    "logits, loss = bi(xt, yt)\n",
    "\n",
    "#idx = a 1x1 tensor, meaning that batch=1, time=1; is a zero-tensor, since we start with the new line character,\n",
    "#which is encoded as 0, a reasonable char to start with \n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bi.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80604de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.979805Z",
     "iopub.status.busy": "2023-10-16T20:47:38.979432Z",
     "iopub.status.idle": "2023-10-16T20:47:38.985578Z",
     "shell.execute_reply": "2023-10-16T20:47:38.984283Z"
    },
    "papermill": {
     "duration": 0.014062,
     "end_time": "2023-10-16T20:47:38.987947",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.973885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(bi.parameters(), lr=0.01) #basically updates the parameters (weights, biases whatever) based on the gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24adda35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:38.998693Z",
     "iopub.status.busy": "2023-10-16T20:47:38.998346Z",
     "iopub.status.idle": "2023-10-16T20:47:56.835902Z",
     "shell.execute_reply": "2023-10-16T20:47:56.834976Z"
    },
    "papermill": {
     "duration": 17.845554,
     "end_time": "2023-10-16T20:47:56.838250",
     "exception": false,
     "start_time": "2023-10-16T20:47:38.992696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.131145477294922\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#training the model\n",
    "for steps in range(10000):\n",
    "    #sampling new data\n",
    "    xt, yt = get_batch('train')\n",
    "    #calculating the loss\n",
    "    logits, loss = bi(xt, yt)\n",
    "    #zeroing all the gradients from the previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #getting the gradients of all the parameters\n",
    "    loss.backward()\n",
    "    #using the gradients to update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f5edfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T20:47:56.850282Z",
     "iopub.status.busy": "2023-10-16T20:47:56.849202Z",
     "iopub.status.idle": "2023-10-16T20:47:56.894600Z",
     "shell.execute_reply": "2023-10-16T20:47:56.893432Z"
    },
    "papermill": {
     "duration": 0.053656,
     "end_time": "2023-10-16T20:47:56.896941",
     "exception": false,
     "start_time": "2023-10-16T20:47:56.843285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " t e a f n w a   r   l , L t t t e y e \n",
      " g g w l u t n t   o a t u t   ' C k y f f e e \n",
      " y d i I l i r n a c r \n",
      "   d d a \n",
      " p o k r   h n ,   e i h a   a t n d   t o v r w     i     a g H w p e   f g h i r     e s a   a y e \n",
      " ! \n",
      "       s o y s g e h   v i r i , o v ; h f c , h a e y I u     e g v h m l s u r r h s   i i h   h s , s N t   ' n s - s e p e   e I h o c   i c e t r e s   I   g e h e h l   e   r   t   g g t   f , - d u   a       s e h f u u ' u   S a h o i   o l d n g d s a e     r     r t   a     g k t i   n e i r e o   n m i d N i n v , m   , s e   n e M t ' d o A   o v e t e i s r\n"
     ]
    }
   ],
   "source": [
    "print(decode(bi.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66564f0",
   "metadata": {
    "papermill": {
     "duration": 0.003601,
     "end_time": "2023-10-16T20:47:56.904373",
     "exception": false,
     "start_time": "2023-10-16T20:47:56.900772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is progress, though, the text is still quite unreasonable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.607459,
   "end_time": "2023-10-16T20:47:57.829150",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-16T20:47:30.221691",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
